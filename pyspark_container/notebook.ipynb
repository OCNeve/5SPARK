{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52599351-c003-46ef-8a06-081693dc0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, length\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType\n",
    "import time \n",
    "\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "# Define schema for incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"favourites\", LongType(), True),\n",
    "    StructField(\"reblogs\", LongType(), True),\n",
    "    StructField(\"hashtags\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MastodonStreamProcessor\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,\"\n",
    "            \"org.apache.kafka:kafka-clients:3.3.1,\"\n",
    "            \"org.postgresql:postgresql:42.2.18\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/spark_checkpoint\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/correct/path/to/log4j.properties\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f199c18-4156-44f9-9ee1-492075bd469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_stream():\n",
    "    # Read from Kafka topic\n",
    "    kafka_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"subscribe\", \"mastodonStream\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Parse the Kafka stream data\n",
    "    parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "\n",
    "    \n",
    "    # Filter for content containing \"AI\"\n",
    "    keyword_filtered_df = parsed_df.filter(col(\"content\").contains(\"AI\"))\n",
    "\n",
    "    # Aggregate by window of 1 hour\n",
    "    windowed_df = keyword_filtered_df \\\n",
    "        .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "        .groupBy(window(col(\"timestamp\"), \"1 hour\")) \\\n",
    "        .count()\n",
    "\n",
    "    # Accessing window.start and window.end\n",
    "    windowed_df = windowed_df \\\n",
    "        .withColumn(\"window_start\", col(\"window.start\")) \\\n",
    "        .withColumn(\"window_end\", col(\"window.end\")) \\\n",
    "        .drop(\"window\")\n",
    "\n",
    "    windowed_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(lambda df, epochId: df.show()) \\  # This will print to the console\n",
    "    .start()\n",
    "\n",
    "    \n",
    "    # Calculate average toot length per user\n",
    "    avg_toot_length_df = keyword_filtered_df \\\n",
    "        .withColumn(\"toot_length\", length(col(\"content\"))) \\\n",
    "        .groupBy(\"user_id\") \\\n",
    "        .agg({\"toot_length\": \"avg\"}) \\\n",
    "        .withColumnRenamed(\"avg(toot_length)\", \"avg_toot_length\")\n",
    "    \n",
    "    # Write windowed aggregation to PostgreSQL\n",
    "    window_query = windowed_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .foreachBatch(lambda df, epochId: df.write.jdbc(\n",
    "            url=jdbc_url, \n",
    "            table=\"toot_window_counts\", \n",
    "            mode=\"append\", \n",
    "            properties=connection_properties)) \\\n",
    "        .start()\n",
    "    \n",
    "    # Write average toot length to PostgreSQL\n",
    "    avg_length_query = avg_toot_length_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .foreachBatch(lambda df, epochId: df.write.jdbc(\n",
    "            url=jdbc_url, \n",
    "            table=\"avg_toot_length\", \n",
    "            mode=\"append\", \n",
    "            properties=connection_properties)) \\\n",
    "        .start()\n",
    "    # For windowed aggregation\n",
    "    window_query = windowed_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "    \n",
    "    # For average toot length\n",
    "    avg_length_query = avg_toot_length_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "\n",
    "    window_query.awaitTermination()\n",
    "    avg_length_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597c2e42-2b03-446f-bfb5-8beb8b1c25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de24a5d9-c86e-4fd5-b138-626730986df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "class TweetSentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self.data = spark.read.csv(\"/home/jovyan/work/data.csv\", header=False, inferSchema=True)\n",
    "\n",
    "        self.data = self.data.withColumnRenamed(\"_c0\", \"target\") \\\n",
    "                   .withColumnRenamed(\"_c1\", \"ids\") \\\n",
    "                   .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "                   .withColumnRenamed(\"_c3\", \"flag\") \\\n",
    "                   .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "                   .withColumnRenamed(\"_c5\", \"text\")\n",
    "    def show_dataset(self):\n",
    "        self.data.show(5)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Tokenize the tweet text\n",
    "        self.tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        tokenized_data = self.tokenizer.transform(self.data)\n",
    "        \n",
    "        # Convert words into feature vectors\n",
    "        self.vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "        self.vectorized_data = self.vectorizer.fit(tokenized_data).transform(tokenized_data)\n",
    "        \n",
    "        # Show the transformed data\n",
    "        self.vectorized_data.select(\"text\", \"words\", \"features\").show(5)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Convert target labels (0 -> negative, 4 -> positive)\n",
    "        self.data = self.data.withColumn(\"label\", (self.data.target / 4).cast(\"int\"))  # Normalize target (0 -> 0, 4 -> 1)\n",
    "        \n",
    "        # Split the dataset into training and testing sets\n",
    "        train_data, self.test_data = self.data.randomSplit([0.8, 0.2], seed=42)\n",
    "        \n",
    "        # Logistic Regression model\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline(stages=[self.tokenizer, self.vectorizer, lr])\n",
    "        \n",
    "        # Train the model\n",
    "        self.model = pipeline.fit(train_data)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        # Make predictions\n",
    "        predictions = self.model.transform(self.test_data)\n",
    "        \n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "        \n",
    "        print(f\"Test Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146c06a7-76c3-426b-a696-971b79669038",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa = TweetSentimentAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1002c5e8-738f-4243-b702-a6882722fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|target|       ids|                date|    flag|           user|                text|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tsa.show_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ecd2c7-6d63-48de-b172-7864b9d7759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|@switchfoot http:...|[@switchfoot, htt...|(262144,[1,2,4,7,...|\n",
      "|is upset that he ...|[is, upset, that,...|(262144,[1,4,6,8,...|\n",
      "|@Kenichan I dived...|[@kenichan, i, di...|(262144,[0,1,2,3,...|\n",
      "|my whole body fee...|[my, whole, body,...|(262144,[5,6,13,3...|\n",
      "|@nationwideclass ...|[@nationwideclass...|(262144,[0,7,18,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tsa.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a83842-8045-416b-9921-699d1f37a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d4bbd3-8b48-4638-aa8a-9bdfda13d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7658921470784228\n"
     ]
    }
   ],
   "source": [
    "tsa.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c1a419-3cb8-4fe3-92fb-5023cc36ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sentiment_analysis(data):\n",
    "    data_predictions = tsa.model.transform(data)\n",
    "    data_predictions.select(\"text\", \"prediction\").write \\\n",
    "        .jdbc(url=jdbc_url, table=\"sentiment_analysis\", mode=\"append\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee53e5b3-908c-422a-bdf2-26fbeda23e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = spark.read.csv(\"/home/jovyan/work/data.csv\", header=False, inferSchema=True)\n",
    "\n",
    "batch_data = batch_data.withColumnRenamed(\"_c0\", \"target\") \\\n",
    "           .withColumnRenamed(\"_c1\", \"ids\") \\\n",
    "           .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "           .withColumnRenamed(\"_c3\", \"flag\") \\\n",
    "           .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "           .withColumnRenamed(\"_c5\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f163cf8-d2ea-4786-9be9-96499f124831",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentiment_analysis(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637ae469-a9f0-4d68-8bc9-3eee20deaee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/conda/lib/python3.11/site-packages (1.39.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (5.1.2)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (1.6.3)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /opt/conda/lib/python3.11/site-packages (from streamlit) (1.24.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/conda/lib/python3.11/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (10.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/conda/lib/python3.11/site-packages (from streamlit) (4.24.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (13.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.11/site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (13.9.2)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (4.8.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.11/site-packages (from streamlit) (3.1.40)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/conda/lib/python3.11/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.11/site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: watchdog<6,>=2.1.5 in /opt/conda/lib/python3.11/site-packages (from streamlit) (5.0.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.19.1)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (3.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4e972-d96a-49ff-8ef0-19779da2445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.32.6:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://185.62.227.171:8501\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02df0f5-13b6-431b-a999-c813532a8e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
